{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4368b6ed-d9ce-4dfc-b344-82cd71571830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f2b103a-1f09-486a-92f6-47c2eeba97c0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "77e57da0-b4e5-41b3-93b4-96eac24b8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool, sum_models\n",
    "import h5py\n",
    "import json\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import optuna\n",
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import sys\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1959a435-aecb-4c29-9e0e-86f48847fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9699d-8caf-4b7c-8987-e7b7f764da02",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86233d9e-c714-4c4c-aa90-81a8d2dd9e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_path = '../configs/settings.json'\n",
    "with open(settings_path, 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ec2ef4f-01ae-482f-a987-77b731b29afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.././data//metadata.h5'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_daily_h5 = f\"../{config['RAW_DATA_DIR']}/daily/\"\n",
    "folder_daily_h5\n",
    "\n",
    "metadata_filename = 'metadata.h5'\n",
    "metadata_filepath = f\"../{config['RAW_DATA_DIR']}/{metadata_filename}\"\n",
    "metadata_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c6a905-2d46-45c4-9307-a4b5d0ae127c",
   "metadata": {},
   "source": [
    "# Daily load trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c6bf44c-4063-4cb9-b11f-91ee1a9b70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata():\n",
    "    with h5py.File(metadata_filepath, 'r') as f:\n",
    "        return f['date_ids'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6cc20fe2-e330-43fb-87fe-b0ab472e49ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_ids = load_metadata()\n",
    "len(date_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18bdf4fc-28c6-446a-908e-2c576bd9d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_daily_minimal(date_id, folder_daily_h5):\n",
    "    filepath=f'{folder_daily_h5}/{date_id}.h5'\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        # Load the target column\n",
    "        target = f['data']['target'][:]\n",
    "\n",
    "        features_group = f['data']['features']\n",
    "        feature_list = [features_group[name][:] for name in features_group.keys()]\n",
    "        features = np.array(feature_list).T \n",
    "\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5993bb-1d80-4f4f-a5a0-02553193e938",
   "metadata": {},
   "source": [
    "# Validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "785c4a9e-06c1-4f48-9982-43eff490834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_size =  60\n",
    "test_size=15\n",
    "slide_step=30\n",
    "n_samples=len(date_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46495f-fb39-4ed2-bc37-6bb59e90cc81",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "effective_train_size = max_train_size + gap\n",
    "n_splits = (n_samples - test_size) // (effective_train_size + test_size) + 1\n",
    "print(n_splits)\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits,\n",
    "                max_train_size=max_train_size, \n",
    "                test_size=test_size, \n",
    "                gap=gap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "18e06c3a-d3a8-4d3b-a62d-c4dc5fd570c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowSplit:\n",
    "    def __init__(self, \n",
    "                 data, \n",
    "                 train_days, \n",
    "                 test_days, \n",
    "                 slide_step=1):\n",
    "        self.data = data\n",
    "        self.train_days = train_days\n",
    "        self.test_days = test_days\n",
    "        self.slide_step = slide_step\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.train_days, len(self.data) - self.test_days + 1, self.slide_step):\n",
    "            yield np.arange(i - self.train_days, i), np.arange(i, i + self.test_days)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - self.train_days - self.test_days + 1) // self.slide_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c4317c3-6ae6-4266-8cf9-0aca2a71d6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = SlidingWindowSplit(date_ids, \n",
    "                       max_train_size, \n",
    "                       test_size, \n",
    "                       slide_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5771cb67-a175-40e9-b85f-d32c1bb5dfc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training and Validation: 14it [00:00, 1701.35it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59]\n",
      "  Test:  index=[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74]\n",
      "Fold 1:\n",
      "  Train: index=[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53\n",
      " 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77\n",
      " 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "  Test:  index=[ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104]\n",
      "Fold 2:\n",
      "  Train: index=[ 60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77\n",
      "  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95\n",
      "  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113\n",
      " 114 115 116 117 118 119]\n",
      "  Test:  index=[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134]\n",
      "Fold 3:\n",
      "  Train: index=[ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149]\n",
      "  Test:  index=[150 151 152 153 154 155 156 157 158 159 160 161 162 163 164]\n",
      "Fold 4:\n",
      "  Train: index=[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155\n",
      " 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173\n",
      " 174 175 176 177 178 179]\n",
      "  Test:  index=[180 181 182 183 184 185 186 187 188 189 190 191 192 193 194]\n",
      "Fold 5:\n",
      "  Train: index=[150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167\n",
      " 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185\n",
      " 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203\n",
      " 204 205 206 207 208 209]\n",
      "  Test:  index=[210 211 212 213 214 215 216 217 218 219 220 221 222 223 224]\n",
      "Fold 6:\n",
      "  Train: index=[180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239]\n",
      "  Test:  index=[240 241 242 243 244 245 246 247 248 249 250 251 252 253 254]\n",
      "Fold 7:\n",
      "  Train: index=[210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227\n",
      " 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245\n",
      " 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263\n",
      " 264 265 266 267 268 269]\n",
      "  Test:  index=[270 271 272 273 274 275 276 277 278 279 280 281 282 283 284]\n",
      "Fold 8:\n",
      "  Train: index=[240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257\n",
      " 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275\n",
      " 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298 299]\n",
      "  Test:  index=[300 301 302 303 304 305 306 307 308 309 310 311 312 313 314]\n",
      "Fold 9:\n",
      "  Train: index=[270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329]\n",
      "  Test:  index=[330 331 332 333 334 335 336 337 338 339 340 341 342 343 344]\n",
      "Fold 10:\n",
      "  Train: index=[300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317\n",
      " 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335\n",
      " 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353\n",
      " 354 355 356 357 358 359]\n",
      "  Test:  index=[360 361 362 363 364 365 366 367 368 369 370 371 372 373 374]\n",
      "Fold 11:\n",
      "  Train: index=[330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347\n",
      " 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365\n",
      " 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383\n",
      " 384 385 386 387 388 389]\n",
      "  Test:  index=[390 391 392 393 394 395 396 397 398 399 400 401 402 403 404]\n",
      "Fold 12:\n",
      "  Train: index=[360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419]\n",
      "  Test:  index=[420 421 422 423 424 425 426 427 428 429 430 431 432 433 434]\n",
      "Fold 13:\n",
      "  Train: index=[390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407\n",
      " 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425\n",
      " 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443\n",
      " 444 445 446 447 448 449]\n",
      "  Test:  index=[450 451 452 453 454 455 456 457 458 459 460 461 462 463 464]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, test_index) in enumerate(tqdm(tscv, \n",
    "                                         desc=\"Training and Validation\")):\n",
    "    \n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70452b9e-21b9-4fad-8527-4ccb96310825",
   "metadata": {},
   "source": [
    "# Intermediate optuna on Catboost for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a4edf-393d-4baf-bd90-5e141f7c9574",
   "metadata": {},
   "source": [
    "## Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7ed66b73-c131-402b-a1c9-231c65be80a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_catboost(trial):\n",
    "\n",
    "    param = {\n",
    "        'iterations' : 400, \n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", \n",
    "      0.001, 0.01),\n",
    "        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", \n",
    "      2, 50),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", \n",
    "      0.01, 0.8),\n",
    "        \n",
    "        \"depth\": trial.suggest_int(\"depth\", 3, 9),\n",
    "        \n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", \n",
    "     [\"Ordered\", \"Plain\"]),\n",
    "        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", \n",
    "     [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n",
    "        \"used_ram_limit\": \"14gb\"\n",
    "    }\n",
    "\n",
    "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", \n",
    "     0, 20)\n",
    "        \n",
    "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", \n",
    "     0.1, 1)\n",
    "        \n",
    "    tscv = SlidingWindowSplit(date_ids, \n",
    "                           max_train_size, \n",
    "                           test_size, \n",
    "                           slide_step)\n",
    "\n",
    "    all_valid_labels = []\n",
    "    all_valid_preds = []\n",
    "    models = []\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tqdm(tscv,\n",
    "                                                       desc=\"Training and Validation\")):\n",
    "        train_date_ids = date_ids[train_index]\n",
    "        test_date_ids = date_ids[test_index]\n",
    "\n",
    "        # Load training data\n",
    "        train_data = []\n",
    "        train_labels = []\n",
    "        for date_id in train_date_ids:\n",
    "            daily_data, daily_labels = load_daily_minimal(date_id, \n",
    "                                                          folder_daily_h5)\n",
    "            \n",
    "            train_data.append(daily_data)\n",
    "            train_labels.append(daily_labels)\n",
    "        \n",
    "        train_data = np.vstack(train_data)\n",
    "        train_labels = np.hstack(train_labels)\n",
    "\n",
    "        # Load validation data\n",
    "        valid_data = []\n",
    "        valid_labels = []\n",
    "        for date_id in test_date_ids:\n",
    "            daily_data, daily_labels = load_daily_minimal(date_id, \n",
    "                                                          folder_daily_h5)\n",
    "    \n",
    "            valid_data.append(daily_data)\n",
    "            valid_labels.append(daily_labels)\n",
    "\n",
    "        valid_data = np.vstack(valid_data)\n",
    "        valid_labels = np.hstack(valid_labels)\n",
    "\n",
    "        # Inputation of NaN\n",
    "        train_labels = np.nan_to_num(train_labels, nan=-9e10)\n",
    "        valid_labels = np.nan_to_num(valid_labels, nan=-9e10)\n",
    "        \n",
    "        batch = Pool(train_data, label=train_labels)\n",
    "\n",
    "        # Train\n",
    "        if i == 0:\n",
    "            model = CatBoostRegressor(**param, \n",
    "                                      thread_count=-1, \n",
    "                                      random_seed=42)\n",
    "        else:\n",
    "            model = CatBoostRegressor(**param, \n",
    "                                      thread_count=-1, \n",
    "                                      random_seed=42)\n",
    "            batch.set_baseline(models[-1].predict(batch))\n",
    "        \n",
    "        model.fit(batch, verbose=0)\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "        preds = model.predict(valid_data)\n",
    "        all_valid_labels.extend(valid_labels)\n",
    "        all_valid_preds.extend(preds)\n",
    "\n",
    "    # https://catboost.ai/en/docs/concepts/python-usages-examples#batch-training\n",
    "    final_model = sum_models(models)\n",
    "\n",
    "    final_preds = final_model.predict(valid_data)\n",
    "    rmse = mean_squared_error(all_valid_labels, \n",
    "                              all_valid_preds, \n",
    "                              squared=False)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17555e8c-320b-4bd3-be38-ddcf831c7e80",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc893f-f312-4408-b39d-81e689b2d27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-05 05:37:38,518] A new study created in memory with name: no-name-d80d3240-4a02-48e6-8246-06f99a30b99e\n",
      "Training and Validation:   8%|â–Š         | 1/13 [08:58<1:47:41, 538.44s/it]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(fit_catboost, \n",
    "               n_trials=120,\n",
    "               timeout = 60*60*4\n",
    "              )\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25231d74-5773-459e-af1a-06feccb3dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f470e4-70df-43dd-85a9-0eb430871634",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188cfa3f-bf9c-4dc4-bdb0-e174232ab6c8",
   "metadata": {},
   "source": [
    "## Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a2810f-d3d7-4c75-8507-fc991d3a80ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame({'feature_importance': model.get_feature_importance(train_pool), \n",
    "              'feature_names': x_val.columns}).sort_values(by=['feature_importance'], \n",
    "                                                           ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046de9b9-4a1a-4531-8281-4ce5b744ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3053c3fd-b25d-435c-ad9b-95181589c900",
   "metadata": {},
   "source": [
    "# Optuna intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2233581-396e-466e-8678-f749011099c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5b4e5d-dca4-484c-9fb3-09b7343d0b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:boosting_mle]",
   "language": "python",
   "name": "conda-env-boosting_mle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
